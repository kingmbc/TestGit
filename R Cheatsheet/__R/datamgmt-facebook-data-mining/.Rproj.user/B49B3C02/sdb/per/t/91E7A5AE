{
    "contents" : "install.packages(\"tm\")\n\nlibrary(twitteR)\nlibrary(KoNLP)\nlibrary(wordcloud)\nlibrary(tm)\n\n# gogamza <- userTimeline(user=\"gogamza\", n=1000)\n# \n# gogamzatw <- c()\n# for(i in 1:length(gogamza)){\n#   gogamzatw <- append(gogamzatw, gogamza[[i]]$text)\n# }\n# \n# gogamzaNoun <- sapply(gogamzatw, extractNoun,USE.NAMES=F)\n# \n# gogamza.corpus <- Corpus(VectorSource(gogamzaNoun))\n# \n# control <- list(stopwords=T, removePunctuation = T, removeNumbers=T, minDocFreq=2)\n# tw.tdm <- TermDocumentMatrix(gogamza.corpus, control)\n\ngogamza <- getUser(\"gogamza\")\n#gogamza.friends <- gogamza$getFriends()\n#gogamza.friendsID <- gogamza$getFriendIDs()\ngogamza.followers <- gogamza$getFollowers()\n\n\n#팔로워들의 자기 소개를 벡터에 적재 한다. \nfollowerDesc <- c()\nfor(i in gogamza.followers){\n  followerDesc <- append(followerDesc, i$description)\n}\n\n\n#쓸모없는 문자들을 제거한다. \nfollowerDesc <- gsub(\"\\n\",\"\", followerDesc)\nfollowerDesc <- gsub(\"\\r\", \"\", followerDesc)\nfollowerDesc <- gsub(\"\\n\",\"\", followerDesc)\nfollowerDesc <- gsub(\"\\r\", \"\", followerDesc)\nfollowerDesc <- gsub(\"co\", \"\", followerDesc)\nfollowerDesc <- gsub(\"http\", \"\", followerDesc)\nfollowerDesc <- gsub(\"!\", \"\", followerDesc)\nfollowerDesc <- gsub(\"?\", \"\", followerDesc)\n#followerDesc <- gsub(\".\", \"\", followerDesc)\nfollowerDesc <- gsub(\"ㅋ\", \"\", followerDesc)\nfollowerDesc <- gsub(\"ㅜ\", \"\", followerDesc)\nfollowerDesc <- gsub(\"ㅠ\", \"\", followerDesc)\nfollowerDesc <- gsub(\"ㅎ\", \"\", followerDesc)\nfollowerDesc <- gsub(\"RT\", \"\", followerDesc)\n\n\nnouns <- Map(extractNoun, followerDesc)\n\nwordsvec <- unlist(nouns, use.name=F)\n#쓸모없는 문자들을 제거한다. 특히 영문자의 경우 tm의 stopwords를 활용한다. \nwordsvec <- wordsvec[-which(wordsvec %in% stopwords(\"english\"))]\nwordsvec <- gsub(\"[[:punct:]]\",\"\", wordsvec)\nwordsvec <- Filter(function(x){nchar(x)>=2}, wordsvec)\n\nwordcount <- table(wordsvec)\npal <- brewer.pal(8,\"Dark2\")\n\n\nwordcloud(names(wordcount),freq=wordcount,scale=c(4,0.5),min.freq=10,\n          random.order=T,rot.per=.1,colors=pal)\n##############################################################################\n\nlibrary(twitteR)\nlibrary(KoNLP)\nlibrary(wordcloud)\nlibrary(tm)\n\nn1=100\nnn <- c(1:n1)\nkeywd<-\"대통령\"\nkeywd<-enc2utf8(keywd)\nresult<-searchTwitter(keywd, n=n1)\nresulttext <- c()\nfor(i in nn){\n  resulttext <- append(resulttext, enc2utf8(result[[i]]$text))\n}\n\nresulttext <- gsub(\"\\n\",\"\", resulttext)\nresulttext <- gsub(\"\\r\", \"\", resulttext)\nresulttext <- gsub(\"co\", \"\", resulttext)\nresulttext <- gsub(\"http\", \"\", resulttext)\nresulttext <- gsub(\"!\", \"\", resulttext)\nresulttext <- gsub(\"?\", \"\", resulttext)\n#resulttext <- gsub(\".\", \"\", resulttext)\nresulttext <- gsub(\"ㅋ\", \"\", resulttext)\nresulttext <- gsub(\"ㅜ\", \"\", resulttext)\nresulttext <- gsub(\"ㅠ\", \"\", resulttext)\nresulttext <- gsub(\"ㅎ\", \"\", resulttext)\nresulttext <- gsub(\"RT\", \"\", resulttext)\n\nnouns <- Map(extractNoun, resulttext)\n\nwordsvec <- unlist(nouns, use.name=F)\n#쓸모없는 문자들을 제거한다. 특히 영문자의 경우 tm의 stopwords를 활용한다.\nwordsvec <- wordsvec[-which(wordsvec %in% stopwords(\"english\"))]\nwordsvec <- gsub(\"[[:punct:]]\",\"\", wordsvec)\nwordsvec <- Filter(function(x){nchar(x)>=2}, wordsvec)\n\nwordcount <- table(wordsvec)\npal <- brewer.pal(8,\"Dark2\")\n\n\nwordcloud(names(wordcount),freq=wordcount,scale=c(4,1.5),min.freq=10, random.order=T,rot.per=.1,colors=pal)\n###############################################################################\n\nlibrary(topicmodels);\nlibrary(lda)\nhelp(package=lda)\n\ndata(\"cora.documents\", package = \"lda\")\ndata(\"cora.vocab\", package = \"lda\")\ndtm <- ldaformat2dtm(cora.documents, cora.vocab)\ncora <- dtm2ldaformat(dtm)\nall.equal(cora, list(documents = cora.documents,\n                     vocab = cora.vocab))\n\ndata(\"AssociatedPress\");\ndata(\"AssociatedPress\", package = \"topicmodels\")\nlda <- LDA(AssociatedPress[1:20,], control = list(alpha = 0.1), k = 2)\nlda_inf <- posterior(lda, AssociatedPress[21:30,])\n\ntxt <- system.file(\"texts\", \"txt\", package = \"tm\")\n(ovid <- Corpus(DirSource(txt),readerControl = list(language = \"lat\")))\nreuters <- Corpus(DirSource(reut21578), readerControl = list(reader = readReut21578XML))\nstopwords(\"english\")\nstopwords(\"SMART\")\n\n###############################################################################\ninstall.packages(\"KoNLP\")\ninstall.packages(\"devtools\")\ninstall_github(\"KoNLP\", \"haven-jeon\",ref=\"KoNLP_0.76.5\")\nlibrary(KoNLP)\nlibrary(wordcloud)\nlibrary(plyr)\nlibrary(devtools)\n\nahn = readLines(\"dat/ahn.txt\")\nahn2 = scan(\"dat/ahn.txt\", character(0)) # separate each word\n\nuseSejongDic()\nuseSystemDic()\n# Backup was just finished!\n# 13 words were added to dic_user.txt.\nmergeUserDic(data.frame(\"삼성전자\", \"ncn\"))\n# 1 words were added to dic_user.txt.\nextractNoun(\"삼성전자에서는 아이폰에 대항할 무기를 준비하고 있다.\")\n# [1] \"삼성전자\" \"아이폰에\" \"대항\"     \"무기\"     \"준비\"    \nmergeUserDic(data.frame(\"아이폰\", \"ncn\"))\n# 1 words were added to dic_user.txt.\nextractNoun(\"삼성전자에서는 아이폰에 대항할 무기를 준비하고 있다.\")\n# [1] \"삼성전자\" \"아이폰\"   \"대항\"     \"무기\"     \"준비\"\n\nhelp.search(keyword = \"text\")\n\nmergeUserDic(data.frame(c('안철수', '박근혜', '문제인'), c('nqpc')))\nnouns <- extractNoun(ahn2)\nnouns <- nouns[nchar(nouns)>=2]\ncnouns <- count(nouns)\n\npal <- brewer.pal(6,\"Dark2\")\npal <- pal[-(1)]\n\nwindowsFonts(malgun=windowsFont(\"맑은 고딕\"))\nwordcloud(words=cnouns$x, freq=cnouns$freq, colors=pal, min.freq=3, \n          random.order=F, family=\"malgun\")\n\n//================================\n  \nrequire(RCurl)\nrequire(rjson)\nkingmbcTok = \"AAACEdEose0cBAJwJsmfKPy8J93MiSwWQYZCSVMXvP0qMIMLjvrdxZCjMJo0fNQ1mng2PyL3EpjczMVUi2yWO1SEVZCDZB2EZD\";\nfacebook <-  function( path = \"me\", access_token = token, options){\n  if( !missing(options) ){\n    options <- sprintf( \"?%s\", paste( names(options), \"=\", unlist(options), collapse = \"&\", sep = \"\" ) )\n  } else {\n    options <- \"\"\n  }\n  data <- getURL( sprintf( \"https://graph.facebook.com/%s%s&access_token=%s\", path, options, access_token ) )\n  fromJSON( data )\n}\n\ndir.create( \"photos\" )\nphotos <- facebook( \"me/photos\", kingmbcTok )\nsapply( photos$data, function(x){\n  url <- x$source\n  download.file( url, file.path( \"photos\", basename(url) ) )\n})\n\n",
    "created" : 1348677754517.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "2150058491",
    "id" : "91E7A5AE",
    "lastKnownWriteTime" : 1348677929,
    "path" : "D:/Kingmbc-Doc/Dropbox/GCT711 Social media analysis/topicmodel.R",
    "properties" : {
    },
    "source_on_save" : false,
    "type" : "r_source"
}